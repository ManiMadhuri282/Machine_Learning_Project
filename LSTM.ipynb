{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tText Prediction | Long Short-Term Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn, Tensorflow, Keras, Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import urllib # to read the data directly from url\n",
    "from urllib import request\n",
    "import re\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import cv2\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.layers import Dropout, MaxPooling2D, Conv2D, Dense\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "#from skimage.color import rgb2grey\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative Models for Text\n",
    "\n",
    "\n",
    "#### Building a generative model to mimic the writing style of prominent British Mathematician, Philosopher, prolific writer, and political activist, Bertrand Russell.\n",
    "\n",
    "#### Data Source : Project Gutenberg http://www.gutenberg.org/ebooks/author/355 in text format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####  LSTM to train the model to predict Russell's writings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('all_Russells.txt','w').close()\n",
    "##function to extract data from the respective urls directly and save to a file\n",
    "def extractText(textUrl):\n",
    "    data = urllib.request.urlopen(textUrl).read().decode('utf-8').replace('\\r', ' ').replace('\\n', ' ').replace(\"\\'\", ' ').replace(\"'\\ufeff\", ' ').replace(\"'\\u03b1'\", ' ')\n",
    "    removed_end = data.split('END OF THIS PROJECT GUTENBERG')[0]\n",
    "    removed_start = removed_end.split(\"START OF THIS PROJECT GUTENBERG EBOOK\")[1]\n",
    "    return(str(removed_start.encode(\"utf-8\")))\n",
    "\n",
    "\n",
    "all_text = open('all_Russells.txt', 'a')\n",
    "phylosophy = extractText('http://www.gutenberg.org/cache/epub/5827/pg5827.txt')\n",
    "analysis = extractText('http://www.gutenberg.org/cache/epub/2529/pg2529.txt')\n",
    "knowledge = extractText('http://www.gutenberg.org/files/37090/37090-0.txt')\n",
    "all_text.write(phylosophy)\n",
    "all_text.write(analysis)\n",
    "all_text.write(knowledge)\n",
    "all_text.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character-level representation for this model by using extended ASCII that has N = 256 characters.\n",
    "#### Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0; 1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Russells_data = open('all_Russells.txt').read()\n",
    "all_Russells_data = all_Russells_data.lower()\n",
    "\n",
    "sorted_list_set = sorted(list(set(all_Russells_data)))\n",
    "int_char = dict((x, p) for p, x in enumerate(sorted_list_set))\n",
    "sorted_list_set_number = len(all_Russells_data)\n",
    "count_sorted_list_set = len(sorted_list_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window size, e.g., W = 100.\n",
    "#### Inputs to the network will be the first W âˆ’1 = 99 characters of each sequence, and the output of the network will be the Wth character of the sequence.Training the network to predict each character using the 99 characters that precede it. Slide the window in strides of S = 1 on the text. For example, if W = 5 and S = 1 and we want to train the network with the sequence ABRACADABRA, The first input to the network will be ABRA and the corresponding output will be C. The second input will be BRAC and the second output will be A, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "strider_window = 100\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, sorted_list_set_number - strider_window, 1):\n",
    "    \n",
    "    seq_in = all_Russells_data[i:i + strider_window]\n",
    "    #print(\"seq_in {} strider_window {} \".format(seq_in,strider_window))\n",
    "    seq_out = all_Russells_data[i + strider_window]\n",
    "    #print(\"seq_out {} ,strider_window {}  \".format(seq_in,strider_window))\n",
    "    X.append([int_char[char] for char in seq_in])\n",
    "    Y.append(int_char[seq_out])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_text_count = len(X)\n",
    "#print(\"matched {}  \".format(matched_text_count))\n",
    "\n",
    "y = np_utils.to_categorical(Y)\n",
    "#print(\"y {}  \".format(y))\n",
    "X = numpy.reshape(X, (matched_text_count, strider_window, 1))\n",
    "X = X / float(count_sorted_list_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "#### Using Softmax output layer to yield a probability prediction for each of the characters between 0 and 1. This is actually a character classification problem with N classes. Choose log loss (cross entropy) as the objective function for the network (research what it means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1209000/1209637 [============================>.] - ETA: 1s - loss: 2.8234Epoch 00001: loss improved from inf to 2.82332, saving model to epoch_loss-01-2.8233.hdf5\n",
      "1209637/1209637 [==============================] - 2407s 2ms/step - loss: 2.8233\n",
      "Epoch 2/10\n",
      "1209000/1209637 [============================>.] - ETA: 0s - loss: 2.6640Epoch 00002: loss improved from 2.82332 to 2.66396, saving model to epoch_loss-02-2.6640.hdf5\n",
      "1209637/1209637 [==============================] - 1868s 2ms/step - loss: 2.6640\n",
      "Epoch 3/10\n",
      "1209000/1209637 [============================>.] - ETA: 0s - loss: 2.5752Epoch 00003: loss improved from 2.66396 to 2.57519, saving model to epoch_loss-03-2.5752.hdf5\n",
      "1209637/1209637 [==============================] - 1806s 1ms/step - loss: 2.5752\n",
      "Epoch 4/10\n",
      "1209000/1209637 [============================>.] - ETA: 1s - loss: 2.4837Epoch 00004: loss improved from 2.57519 to 2.48361, saving model to epoch_loss-04-2.4836.hdf5\n",
      "1209637/1209637 [==============================] - 2372s 2ms/step - loss: 2.4836\n",
      "Epoch 5/10\n",
      "1209000/1209637 [============================>.] - ETA: 1s - loss: 2.3891Epoch 00005: loss improved from 2.48361 to 2.38907, saving model to epoch_loss-05-2.3891.hdf5\n",
      "1209637/1209637 [==============================] - 2064s 2ms/step - loss: 2.3891\n",
      "Epoch 6/10\n",
      "1209000/1209637 [============================>.] - ETA: 1s - loss: 2.3012Epoch 00006: loss improved from 2.38907 to 2.30120, saving model to epoch_loss-06-2.3012.hdf5\n",
      "1209637/1209637 [==============================] - 2584s 2ms/step - loss: 2.3012\n",
      "Epoch 7/10\n",
      "1209000/1209637 [============================>.] - ETA: 1s - loss: 2.2359Epoch 00007: loss improved from 2.30120 to 2.23593, saving model to epoch_loss-07-2.2359.hdf5\n",
      "1209637/1209637 [==============================] - 3183s 3ms/step - loss: 2.2359\n",
      "Epoch 8/10\n",
      "1209000/1209637 [============================>.] - ETA: 0s - loss: 2.1835Epoch 00008: loss improved from 2.23593 to 2.18350, saving model to epoch_loss-08-2.1835.hdf5\n",
      "1209637/1209637 [==============================] - 1825s 2ms/step - loss: 2.1835\n",
      "Epoch 9/10\n",
      "1209000/1209637 [============================>.] - ETA: 0s - loss: 2.1393Epoch 00009: loss improved from 2.18350 to 2.13926, saving model to epoch_loss-09-2.1393.hdf5\n",
      "1209637/1209637 [==============================] - 1824s 2ms/step - loss: 2.1393\n",
      "Epoch 10/10\n",
      "1209000/1209637 [============================>.] - ETA: 0s - loss: 2.0994Epoch 00010: loss improved from 2.13926 to 2.09940, saving model to epoch_loss-10-2.0994.hdf5\n",
      "1209637/1209637 [==============================] - 1828s 2ms/step - loss: 2.0994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ae00b92dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(y.shape[1], activation='softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "location=\"epoch_loss-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "model_checkpoint = ModelCheckpoint(location, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [model_checkpoint]\n",
    "\n",
    "lstm_model.fit(X, y, epochs=10, batch_size=1000, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We do not use a test dataset. We are using the whole training dataset to learn the probability of each character in a sequence. We are not seeking for a very accurate model. Instead we are interested in a generalization of the dataset that can mimic the gist of the text.\n",
    "#### Choosing a reasonable number of epochs4 for training, considering the computational power (e.g., 30, although the network will need more epochs to yield a better model).\n",
    "####  Using  model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Finding the best set of weights in terms of loss.\n",
    "#### Using the network with the best weights to generate 1000 characters, using the following text as initialization of the network: There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" there are those who take mental phenomena naively, just as they would physical phenomena. this schoo \"\n",
      "  th the that the semse of the soetere ooreoem of the soetere ohreop of  the saee to the poososition of the soetere ohreop of the soetere oore of  the seesed that the same thing  and the soetere ohreop of the soetere  ant aalinten                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  "
     ]
    }
   ],
   "source": [
    "\n",
    "# load the network weights\n",
    "filename = \"epoch_loss-10-2.0994.hdf5\"\n",
    "lstm_model.load_weights(filename)\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "sorted_list_int = dict((i, c) for i, c in enumerate(sorted_list_set))\n",
    "\n",
    "\n",
    "p = \"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\".lower()\n",
    "matched_text = [int_char[char] for char in p]\n",
    "matched_text = matched_text[0:100]\n",
    "print(\"\\\"\", ''.join([sorted_list_int[p] for p in matched_text]), \"\\\"\")\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(matched_text, (1, len(matched_text), 1))\n",
    "    x = x / float(count_sorted_list_set)\n",
    "    pred = lstm_model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    res = sorted_list_int[index]\n",
    "    sys.stdout.write(res)\n",
    "    matched_text.append(index)\n",
    "    matched_text = matched_text[1:len(matched_text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
